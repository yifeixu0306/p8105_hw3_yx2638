---
title: "p8105_hw3_yx2638"
author: "Yifei Xu"
date: "2022-10-15"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8,
	fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

#### Read in the data

```{r, message=FALSE}
data("instacart")

instacart = 
        instacart %>% 
        as_tibble(instacart)
```

#### Answer questions about the data

The dataset `instacart` contains `r nrow(instacart)` rows and each row respresents a single product from an instacart order. `r ncol(instacart)` variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. 

In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

#### Make a plot that shows the number of items ordered in each aisle

```{r}
instacart %>% 
        count(aisle) %>% 
        arrange(desc(n))

instacart %>% 
        count(aisle) %>% 
        filter(n > 10000) %>% 
        mutate(aisle = fct_reorder(aisle, n)) %>% 
        ggplot(aes(x = aisle, y = n)) + 
        geom_point() + 
        labs(title = "Number of items ordered in each aisle") +
        theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far. The plot shows the number of items ordered in each aisle and aisles are ordered by ascending number of items.

#### Make a table 

showing the three most popular items in each of the aisles and order times

```{r}
instacart %>% 
        filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
        group_by(aisle) %>% 
        count(product_name) %>% 
        mutate(rank = min_rank(desc(n))) %>% 
        filter(rank < 4) %>% 
        arrange(desc(n)) %>%
        knitr::kable()
```

#### Make a table 

showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered

```{r}
instacart %>%
        filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
        group_by(product_name, order_dow) %>%
        summarize(mean_hour = mean(order_hour_of_day)) %>%
        spread(key = order_dow, value = mean_hour) %>%
        knitr::kable(digits = 2)
```

Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.


# Problem 2

#### Read in the data 

```{r, message=FALSE}
accel = read_csv("data/accel_data.csv") 
```

#### Tidy and wrangle the data

```{r, message=FALSE}
accel_tidy = accel %>%
        janitor::clean_names() %>%
        pivot_longer(activity_1:activity_1440, 
                     names_to = "minute", 
                     values_to = "activity_counts", 
                     names_prefix =  "activity_") %>%
        mutate(day = day %>% 
                       fct_relevel("Monday", "Tuesday", "Wednesday","Thursday","Friday", "Saturday", "Sunday")) %>% 
        mutate(weekday_vs_weekend = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>%
        mutate(minute = as.integer(minute)) 
```

The resulting dataset contains `r nrow(accel_tidy)` observations. `r ncol(accel_tidy)` key variables include `r names(accel_tidy)`. Each observation represent a male's activity count per minute of each day during five weeks.


#### Create a table to show the total activity counts

```{r, message=FALSE}
# aggregate across minutes to create a total activity variable for each day, and create a table showing these totals
accel_tidy %>%
        group_by(week, day) %>%
        rename(Week = week) %>%
        summarize(total_activity_counts = sum(activity_counts)) %>%
        spread(key = day, total_activity_counts) %>%
        knitr::kable(digits = 0)
```

We cannot observe apparent trend from the table. However, compared with weekends, total activity counts are more stable during weekdays and it's obvious that the total activity accounts on Saturdays in week 4 and week 5 are far lower than any other days. 

#### Make a single-panel plot that shows the 24-hour activity time courses for each day

```{r, message=FALSE}
accel_tidy %>%
        group_by(day, minute) %>%
        rename(Day = day) %>%
        ggplot(aes(x = minute, y = activity_counts)) +
        geom_point(aes(color = Day), alpha = 0.6) +
        scale_color_brewer(palette = "Set1") +
        geom_smooth() + 
        scale_x_continuous(
                breaks = c(0, 180, 360, 540, 720, 900, 1080, 1260, 1440),
                labels = c("12am", "3am", "6am", "9am", "12pm", "3pm", "6pm", "9pm", "12am")) + 
        labs(x = "Time", 
             y = "Activity Counts", 
             title = "24-hour Activity Time Courses for Each Day")
```


Within a day, we can observe that the activity count of this male is very low between 12am and 3am, and then begins to increase. Around 9-12am, the activity count reaches the first peak, and after then it remains with little fluctuation. The second peak occurs around 9pm, after which the activity account begins to decline.

Within weekday, his activity count is relatively high around 7am on some Thursdays and around 9am on some Fridays. As for the weekend, the activity count is high in the afternoon, especially around 4-5pm on Saturday and around 12pm on Sunday. Between 8-10pm, the count is usually high across many days, especially on Friday.

In summary, activity count per minute is below 2500 for most of the time throughout a day. From this plot, we can roughly identify his daily schedule (such as the time of wake-up and sleep).


# Problem 3

#### Read in the data

```{r, message=FALSE}
data("ny_noaa")

ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa)
```

#### Tidy and wrangle the data

```{r, message=FALSE}
ny_noaa_tidy = ny_noaa %>%
        janitor::clean_names() %>%
        separate(date, into = c("year", "month", "day")) %>%
        arrange(year, month) %>%
        mutate(year = as.numeric(year),
               month = month.name[as.numeric(month)],
               day = as.numeric(day)) %>%
        mutate(prcp = prcp/10,
               tmax = as.numeric(tmax)/10,
               tmin = as.numeric(tmin)/10) 

```

`NY NOAA` collected weather data all New York state weather stations from January 1, 1981 through December 31, 2010. The tidy dataset has `r nrow(ny_noaa_tidy)` observations, with `r ncol(ny_noaa_tidy)` key variables, including `r names(ny_noaa_tidy)`. 

The `id` is a `r class(ny_noaa_tidy$id)` variable. We separate the date into `year`, `month` and `day`, which are `r class(ny_noaa_tidy$year)`, `r class(ny_noaa_tidy$month)`, `r class(ny_noaa_tidy$day)` variable, respectively. `prcp`, `snow`, `snwd`, `tmax`, `tmin` are unified as `r class(ny_noaa_tidy$prcp)` variables after data cleaning. The unit for precipitation, snowfall and snow depth is unified as "mm", while the unit for maximum temperature `tmax` and minimum temperature `tmin` is both "degrees C".

Each weather station may only collect a subset of these variables, which can lead to problems with missing data in the dataset. Missing data occurs in precipitation, snowfall, snow depth, maximum temperature and minimum temperature, accounting for `r sum(is.na(ny_noaa_tidy$prcp) / nrow(ny_noaa_tidy)) %>% scales::percent(0.01)`, `r sum(is.na(ny_noaa_tidy$snow) / nrow(ny_noaa_tidy)) %>% scales::percent(0.01)`, `r sum(is.na(ny_noaa_tidy$snwd) / nrow(ny_noaa_tidy)) %>% scales::percent(0.01)`, `r sum(is.na(ny_noaa_tidy$tmax) / nrow(ny_noaa_tidy)) %>% scales::percent(0.01)` and `r sum(is.na(ny_noaa_tidy$tmin) / nrow(ny_noaa_tidy)) %>% scales::percent(0.01)`, respectively. We cannot ignore this issue due to the large proportion. The sample is not very representative and the results of our analysis might be less convincing.


#### Identify the most commonly observed values for snowfall

```{r, message=FALSE}
ny_noaa_tidy %>%
        group_by(snow) %>%
        summarize(num = n()) %>%
        arrange(desc(num))

```

For snowfall, the most commonly observed value is 0, which appears 2008508 times. This is because snowfall is an infrequent occurrence in New York state. It does not snow for most of the time throughout the whole year due to the geographical location.


#### Make a two-panel plot showing the average max temperature

```{r, message=FALSE}
# Show the average max temperature in January and in July in each station across years
Jan_Jul = ny_noaa_tidy %>%
        filter(month %in% c("January", "July")) %>%
        drop_na(tmax) %>%
        group_by(id, year, month) %>%
        summarize(avg_tmax = mean(tmax))


Jan_Jul %>%
        ggplot(aes(x = year, y = avg_tmax)) + 
        geom_point(alpha = 0.6) + 
        geom_smooth() +
        facet_grid(. ~ month) +
        labs(x = "Year",
             y = "Average Max Temperature (C)",
             title = "Average Max Temperature in January and July Across Years",
             caption = "Data from the rnoaa package.
             Each point represents the average max temperature of a station at the given time") +
        theme(plot.title = element_text(size = 12)) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text = element_text(size = 10))
        

```

From the two-panel plot, we can intuitively see that the average max temperature in January is much slower than that in July. As is known to all, New York is in the northern hemisphere and January is still in winter and is much colder than July, which is in summer. The average max temperature in January fluctuates greatly across years while in July, the average max temperature is relatively stable across years. From my point of view, that extreme weather often occurs in winter might account for the pattern.

In addition, outliers exist in both January and July, which might be some abnormally extreme low temperature or misreport. 


#### Make another two-panel plot

```{r, message=FALSE}
# (i) show tmax vs tmin for the full dataset
tmax_tmin = ny_noaa_tidy %>%
        drop_na(tmin, tmax) %>%
        ggplot(aes(x = tmin, y = tmax)) +
        geom_hex() + 
        labs(x = "Minimum Temperature (C)", 
             y = "Maximum Temperature (C)",
             title = "Comparison of Max and Min Temperature") +
        theme(plot.title = element_text(size = 12)) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text = element_text(size = 10)) +
        theme(legend.text = element_text(size = 5))


# (ii) show the distribution of snowfall values greater than 0 and less than 100 separately by year
snow_dist = ny_noaa_tidy %>%
        filter(snow < 100 & snow > 0) %>%
        ggplot(aes(x = snow, y = as.factor(year))) +
        geom_density_ridges(scale = 0.6) + 
        labs(x = "Snowfall Values (mm)",
             y = "Year",
             title = "Distribution of Snowfall Values",
             caption = "Data from the rnoaa package") +
        theme(plot.title = element_text(size = 12)) +
        theme(axis.title = element_text(size = 10)) +
        theme(axis.text = element_text(size = 10))

# patch these two plots
tmax_tmin + snow_dist 
    
   
```

From this two-panel plot, we can observe that the maximal and minimal temperature may have a positive correlation and the temperature is around 0 to 25 degrees C for most of the time. The distribution of snowfall values (0-100mm) is quite similar across the years. The snowfall values concentrate between 0 and 30mm and can reach 50-80mm under some extreme weather conditions, which is rare.



